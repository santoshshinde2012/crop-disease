{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b1ece6",
   "metadata": {},
   "source": [
    "# Crop Disease Classification\n",
    "\n",
    "**Objective:** Build a machine learning model to classify plant diseases from leaf images using transfer learning with PyTorch.  \n",
    "**Dataset:** PlantVillage (12 selected classes across Tomato, Potato, and Pepper crops)  \n",
    "**Models:** ResNet-50, EfficientNet-B0, MobileNetV3-Small  \n",
    "**Author:** Santosh Shinde  \n",
    "**Date:** February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78e251",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup\n",
    "Import all modules, set seed, detect device, print environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "# Project modules\n",
    "from src.config import Config, DataConfig, TrainConfig, ModelConfig\n",
    "from src.utils.seed import set_seed\n",
    "from src.data.dataset import PlantDiseaseDataset\n",
    "from src.data.transforms import get_train_transforms, get_val_transforms\n",
    "from src.data.splitter import create_stratified_split\n",
    "from src.data.loader import create_dataloaders, SplitDataset\n",
    "from src.utils.text_helpers import shorten_class_name, get_crop_name\n",
    "from src.utils.plot_data import (\n",
    "    plot_sample_images, plot_class_distribution, plot_augmentation_examples,\n",
    ")\n",
    "from src.utils.plot_training import plot_training_curves, plot_model_comparison\n",
    "from src.models.factory import get_model, count_parameters, get_differential_lr_params\n",
    "from src.models.freeze import freeze_backbone, partial_unfreeze, full_unfreeze\n",
    "from src.training.trainer import Trainer\n",
    "from src.evaluation.metrics import (\n",
    "    compute_predictions, generate_classification_report, compute_summary_metrics\n",
    ")\n",
    "from src.evaluation.confusion import plot_confusion_matrix\n",
    "from src.evaluation.predictions import get_prediction_examples, plot_prediction_grid\n",
    "from src.evaluation.profiler import profile_model\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 150\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Torchvision: {torchvision.__version__}')\n",
    "print(f'NumPy: {np.__version__}')\n",
    "print(f'Project Root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af90a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(f'Using CUDA: {torch.cuda.get_device_name(0)}')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "    print('Using Apple MPS (Metal Performance Shaders)')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print('Using CPU')\n",
    "\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Update data directory to point to actual dataset\n",
    "# Adjust this path based on your setup\n",
    "DATASET_ROOT = PROJECT_ROOT.parent / 'PlantVillage Dataset' / 'PlantVillage'\n",
    "if not DATASET_ROOT.exists():\n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        PROJECT_ROOT / 'data' / 'raw' / 'PlantVillage',\n",
    "        Path('../PlantVillage Dataset/PlantVillage'),\n",
    "    ]\n",
    "    for p in alt_paths:\n",
    "        if p.exists():\n",
    "            DATASET_ROOT = p\n",
    "            break\n",
    "\n",
    "config.data.raw_data_dir = DATASET_ROOT\n",
    "print(f'Dataset root: {DATASET_ROOT}')\n",
    "print(f'Dataset exists: {DATASET_ROOT.exists()}')\n",
    "print(f'Selected classes ({len(config.data.selected_classes)}):')\n",
    "for cls in config.data.selected_classes:\n",
    "    print(f'  - {cls}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1af866",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Exploration (Part 1)\n",
    "\n",
    "Load and visualize the dataset, analyze class distribution, and extract key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset without transforms for visualization\n",
    "dataset = PlantDiseaseDataset(\n",
    "    root_dir=config.data.raw_data_dir,\n",
    "    selected_classes=config.data.selected_classes,\n",
    "    transform=None,  # Raw images for exploration\n",
    ")\n",
    "\n",
    "print(f'Total images: {len(dataset):,}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'\\nClass-to-Index mapping:')\n",
    "for cls, idx in dataset.class_to_idx.items():\n",
    "    print(f'  {idx:2d}: {cls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "class_counts = dataset.get_class_counts()\n",
    "counts_series = pd.Series(class_counts)\n",
    "\n",
    "print('\\nPer-class statistics:')\n",
    "print(f'  Total images: {counts_series.sum():,}')\n",
    "print(f'  Min count:    {counts_series.min():,} ({counts_series.idxmin()})')\n",
    "print(f'  Max count:    {counts_series.max():,} ({counts_series.idxmax()})')\n",
    "print(f'  Mean count:   {counts_series.mean():,.0f}')\n",
    "print(f'  Std count:    {counts_series.std():,.0f}')\n",
    "print(f'  Imbalance ratio (max/min): {counts_series.max() / counts_series.min():.1f}x')\n",
    "\n",
    "# Display as DataFrame\n",
    "df_counts = pd.DataFrame({\n",
    "    'Class': [shorten_class_name(k) for k in class_counts.keys()],\n",
    "    'Full Name': list(class_counts.keys()),\n",
    "    'Count': list(class_counts.values()),\n",
    "    'Crop': [get_crop_name(k) for k in class_counts.keys()],\n",
    "})\n",
    "df_counts = df_counts.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5153a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5x5 sample images grid\n",
    "(PROJECT_ROOT / 'outputs').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig = plot_sample_images(\n",
    "    dataset,\n",
    "    num_classes=5,\n",
    "    images_per_class=5,\n",
    "    figsize=(20, 20),\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'sample_images_grid.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c424bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution bar chart\n",
    "fig = plot_class_distribution(\n",
    "    class_counts,\n",
    "    figsize=(12, 8),\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'class_distribution.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20559227",
   "metadata": {},
   "source": [
    "### Key Insights from Data Exploration\n",
    "\n",
    "**Insight 1 — Class Imbalance:**  \n",
    "There is significant class imbalance in the dataset. The largest class (Tomato Bacterial Spot with ~2,127 images) has substantially more samples than the smallest class (Potato Healthy with ~152 images), yielding an imbalance ratio of approximately 14:1. This imbalance could bias the model toward majority classes, making stratified splitting and potentially class-weighted loss important considerations.\n",
    "\n",
    "**Insight 2 — Visual Similarity Across Crops:**  \n",
    "Diseases that occur across multiple crops (e.g., Early Blight on both Tomato and Potato, Bacterial Spot on Tomato and Pepper) share similar visual patterns — yellowing, spotting, and necrotic lesions. This cross-crop visual similarity is a key classification challenge that tests whether the model learns crop-specific vs. disease-specific features. Confusion between these pairs is expected and informative.\n",
    "\n",
    "**Insight 3 — Lab-Controlled Image Quality:**  \n",
    "PlantVillage images are captured under controlled laboratory conditions with uniform backgrounds and consistent lighting. While this produces a clean signal for training, it creates a significant domain gap with real-world field photography where images may include variable lighting, complex backgrounds, partial leaf occlusion, and motion blur. This limitation should be addressed through aggressive data augmentation and acknowledged as a deployment consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual similarity insight — show Early Blight comparison across crops\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Visual Similarity: Same Disease Across Crops', fontsize=14)\n",
    "\n",
    "# Tomato Early Blight samples\n",
    "tomato_eb_idx = dataset.class_to_idx.get('Tomato_Early_blight', 0)\n",
    "tomato_eb_samples = [i for i, (_, l) in enumerate(dataset.samples) if l == tomato_eb_idx][:4]\n",
    "for col, idx in enumerate(tomato_eb_samples):\n",
    "    img, _ = dataset[idx]\n",
    "    axes[0, col].imshow(img)\n",
    "    axes[0, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[0, col].set_ylabel('Tomato\\nEarly Blight', fontsize=11, rotation=0, labelpad=80)\n",
    "\n",
    "# Potato Early Blight samples\n",
    "potato_eb_idx = dataset.class_to_idx.get('Potato___Early_blight', 0)\n",
    "potato_eb_samples = [i for i, (_, l) in enumerate(dataset.samples) if l == potato_eb_idx][:4]\n",
    "for col, idx in enumerate(potato_eb_samples):\n",
    "    img, _ = dataset[idx]\n",
    "    axes[1, col].imshow(img)\n",
    "    axes[1, col].axis('off')\n",
    "    if col == 0:\n",
    "        axes[1, col].set_ylabel('Potato\\nEarly Blight', fontsize=11, rotation=0, labelpad=80)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba6fa8",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Pipeline (Part 2a)\n",
    "\n",
    "Create stratified train/val/test splits, show augmentation examples, and build DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de438ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified split\n",
    "splits = create_stratified_split(\n",
    "    samples=dataset.samples,\n",
    "    split_ratios=config.data.split_ratios,\n",
    "    seed=config.data.random_seed,\n",
    ")\n",
    "\n",
    "print('Split sizes:')\n",
    "for split_name, samples in splits.items():\n",
    "    labels = [s[1] for s in samples]\n",
    "    print(f'  {split_name:5s}: {len(samples):,} images ({len(samples)/len(dataset)*100:.1f}%)')\n",
    "\n",
    "# Verify stratification\n",
    "print('\\nStratification verification (class proportions):')\n",
    "for split_name, samples in splits.items():\n",
    "    label_counts = Counter(s[1] for s in samples)\n",
    "    total = len(samples)\n",
    "    print(f'  {split_name}: ', end='')\n",
    "    for idx in sorted(label_counts.keys())[:3]:\n",
    "        print(f'{dataset.idx_to_class[idx][:15]}={label_counts[idx]/total:.2%} ', end='')\n",
    "    print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show augmentation examples\n",
    "train_transform = get_train_transforms(config.data.image_size)\n",
    "val_transform = get_val_transforms(config.data.image_size)\n",
    "\n",
    "# Pick a sample image for augmentation demo\n",
    "sample_path = str(dataset.samples[0][0])\n",
    "fig = plot_augmentation_examples(sample_path, train_transform, num_augmented=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8895d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "dataloaders = create_dataloaders(\n",
    "    splits=splits,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    "    batch_size=config.train.batch_size,\n",
    "    num_workers=config.train.num_workers,\n",
    "    pin_memory=config.train.pin_memory,\n",
    ")\n",
    "\n",
    "# Verify batch shapes\n",
    "for split_name, loader in dataloaders.items():\n",
    "    images, labels = next(iter(loader))\n",
    "    print(f'{split_name:5s}: images={images.shape}, labels={labels.shape}, dtype={images.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad57a16",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training (Part 2b)\n",
    "\n",
    "Train three models using three-stage progressive fine-tuning:\n",
    "1. **Stage 1** -- Feature Extraction (frozen backbone, head only)\n",
    "2. **Stage 2** -- Adaptation (partial unfreeze + head)\n",
    "3. **Stage 3** -- Full Refinement (all parameters, differential LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_three_stages(\n",
    "    model_name: str,\n",
    "    config: Config,\n",
    "    dataloaders: dict,\n",
    "    device: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"Train a model through all three stages.\n",
    "    \n",
    "    Returns combined training history.\n",
    "    \"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {model_name.upper()}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Initialize model\n",
    "    model = get_model(\n",
    "        name=model_name,\n",
    "        num_classes=config.model.num_classes,\n",
    "        pretrained=config.model.pretrained,\n",
    "        dropout=config.model.dropout,\n",
    "    ).to(device)\n",
    "    \n",
    "    combined_history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "        'lr': [], 'epoch_time': [],\n",
    "    }\n",
    "    \n",
    "    # ---- STAGE 1: Feature Extraction ----\n",
    "    print(f'\\n--- Stage 1: Feature Extraction (frozen backbone) ---')\n",
    "    freeze_backbone(model, model_name)\n",
    "    params = count_parameters(model)\n",
    "    print(f'Trainable: {params[\"trainable\"]:,} / {params[\"total\"]:,} ({params[\"trainable\"]/params[\"total\"]*100:.1f}%)')\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        num_classes=config.model.num_classes,\n",
    "        learning_rate=config.train.stage1_lr,\n",
    "        weight_decay=config.train.weight_decay,\n",
    "        label_smoothing=config.train.label_smoothing,\n",
    "        device=device,\n",
    "        checkpoint_dir=config.train.checkpoint_dir,\n",
    "        model_name=model_name,\n",
    "        max_grad_norm=config.train.max_grad_norm,\n",
    "        use_amp=config.train.use_amp,\n",
    "    )\n",
    "    \n",
    "    history1 = trainer.fit(\n",
    "        dataloaders['train'], dataloaders['val'],\n",
    "        num_epochs=config.train.stage1_epochs,\n",
    "        scheduler_type='cosine',\n",
    "        patience=config.train.early_stopping_patience,\n",
    "    )\n",
    "    \n",
    "    for key in combined_history:\n",
    "        combined_history[key].extend(history1[key])\n",
    "    \n",
    "    # ---- STAGE 2: Adaptation ----\n",
    "    print(f'\\n--- Stage 2: Adaptation (partial unfreeze) ---')\n",
    "    partial_unfreeze(model, model_name)\n",
    "    params = count_parameters(model)\n",
    "    print(f'Trainable: {params[\"trainable\"]:,} / {params[\"total\"]:,} ({params[\"trainable\"]/params[\"total\"]*100:.1f}%)')\n",
    "    \n",
    "    # Re-initialize optimizer for newly unfrozen parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        num_classes=config.model.num_classes,\n",
    "        learning_rate=config.train.stage2_lr,\n",
    "        weight_decay=config.train.weight_decay,\n",
    "        label_smoothing=config.train.label_smoothing,\n",
    "        device=device,\n",
    "        checkpoint_dir=config.train.checkpoint_dir,\n",
    "        model_name=model_name,\n",
    "        max_grad_norm=config.train.max_grad_norm,\n",
    "        use_amp=config.train.use_amp,\n",
    "    )\n",
    "    # Carry over best F1 from stage 1\n",
    "    trainer.best_val_f1 = max(history1['val_f1']) if history1['val_f1'] else 0.0\n",
    "    \n",
    "    history2 = trainer.fit(\n",
    "        dataloaders['train'], dataloaders['val'],\n",
    "        num_epochs=config.train.stage2_epochs,\n",
    "        scheduler_type='cosine',\n",
    "        patience=config.train.early_stopping_patience,\n",
    "    )\n",
    "    \n",
    "    for key in combined_history:\n",
    "        combined_history[key].extend(history2[key])\n",
    "    \n",
    "    # ---- STAGE 3: Full Refinement ----\n",
    "    print(f'\\n--- Stage 3: Full refinement (all parameters, differential LR) ---')\n",
    "    full_unfreeze(model)\n",
    "    params = count_parameters(model)\n",
    "    print(f'Trainable: {params[\"trainable\"]:,} / {params[\"total\"]:,} ({params[\"trainable\"]/params[\"total\"]*100:.1f}%)')\n",
    "    \n",
    "    # Differential LR: backbone gets lower LR\n",
    "    param_groups = get_differential_lr_params(\n",
    "        model, model_name,\n",
    "        backbone_lr=config.train.stage3_lr,\n",
    "        head_lr=config.train.stage3_lr * 5,  # 5× higher for head\n",
    "    )\n",
    "    \n",
    "    # Re-initialize optimizer with differential LR\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        num_classes=config.model.num_classes,\n",
    "        weight_decay=config.train.weight_decay,\n",
    "        label_smoothing=config.train.label_smoothing,\n",
    "        device=device,\n",
    "        checkpoint_dir=config.train.checkpoint_dir,\n",
    "        model_name=model_name,\n",
    "        max_grad_norm=config.train.max_grad_norm,\n",
    "        use_amp=config.train.use_amp,\n",
    "        param_groups=param_groups,\n",
    "    )\n",
    "    # Carry over best F1 from stage 2\n",
    "    prev_best = max(\n",
    "        max(history1['val_f1']) if history1['val_f1'] else 0.0,\n",
    "        max(history2['val_f1']) if history2['val_f1'] else 0.0\n",
    "    )\n",
    "    trainer.best_val_f1 = prev_best\n",
    "    \n",
    "    history3 = trainer.fit(\n",
    "        dataloaders['train'], dataloaders['val'],\n",
    "        num_epochs=config.train.stage3_epochs,\n",
    "        scheduler_type='cosine',\n",
    "        patience=config.train.early_stopping_patience,\n",
    "    )\n",
    "    \n",
    "    for key in combined_history:\n",
    "        combined_history[key].extend(history3[key])\n",
    "    \n",
    "    print(f'\\n{model_name.upper()} training complete!')\n",
    "    print(f'Best val F1: {trainer.best_val_f1:.4f}')\n",
    "    \n",
    "    return combined_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69255ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three models\n",
    "model_names = ['resnet50', 'efficientnet_b0', 'mobilenetv3']\n",
    "all_histories = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    history = train_model_three_stages(model_name, config, dataloaders, DEVICE)\n",
    "    all_histories[model_name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all models\n",
    "stage_boundaries = [\n",
    "    config.train.stage1_epochs,\n",
    "    config.train.stage1_epochs + config.train.stage2_epochs,\n",
    "]\n",
    "\n",
    "fig = plot_training_curves(\n",
    "    all_histories,\n",
    "    stage_boundaries=stage_boundaries,\n",
    "    figsize=(16, 12),\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'training_curves.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea09ca2",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluation (Part 3a)\n",
    "\n",
    "Load best checkpoints, generate predictions on test set, and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82df51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "all_metrics = {}\n",
    "all_predictions = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Evaluating {model_name.upper()}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Load best checkpoint\n",
    "    model = get_model(\n",
    "        name=model_name,\n",
    "        num_classes=config.model.num_classes,\n",
    "        pretrained=False,\n",
    "        dropout=config.model.dropout,\n",
    "    )\n",
    "    \n",
    "    checkpoint_path = config.train.checkpoint_dir / f'{model_name}_best.pth'\n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f'Loaded checkpoint from epoch {checkpoint[\"epoch\"]} (val_f1={checkpoint[\"val_f1\"]:.4f})')\n",
    "    else:\n",
    "        print(f'WARNING: No checkpoint found at {checkpoint_path}')\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate predictions\n",
    "    preds, labels, probs = compute_predictions(model, dataloaders['test'], DEVICE)\n",
    "    all_predictions[model_name] = (preds, labels, probs)\n",
    "    \n",
    "    # Classification report\n",
    "    class_names = [dataset.idx_to_class[i] for i in range(config.model.num_classes)]\n",
    "    report = generate_classification_report(labels, preds, class_names)\n",
    "    print(f'\\nClassification Report:')\n",
    "    print(report)\n",
    "    \n",
    "    # Summary metrics\n",
    "    metrics = compute_summary_metrics(labels, preds)\n",
    "    all_metrics[model_name] = metrics\n",
    "    print(f'Summary: Accuracy={metrics[\"accuracy\"]:.4f}, F1 Macro={metrics[\"f1_macro\"]:.4f}, F1 Weighted={metrics[\"f1_weighted\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f469f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for each model\n",
    "class_names = [dataset.idx_to_class[i] for i in range(config.model.num_classes)]\n",
    "\n",
    "for model_name in model_names:\n",
    "    preds, labels, _ = all_predictions[model_name]\n",
    "    fig = plot_confusion_matrix(\n",
    "        labels, preds, class_names,\n",
    "        normalize=True,\n",
    "        save_path=PROJECT_ROOT / 'outputs' / f'confusion_matrix_{model_name}.png',\n",
    "        figsize=(12, 10),\n",
    "    )\n",
    "    plt.suptitle(f'Confusion Matrix -- {model_name}', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fff47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Correct and 5 Incorrect predictions for best model\n",
    "best_model_name = max(all_metrics, key=lambda k: all_metrics[k]['f1_macro'])\n",
    "print(f'Best model: {best_model_name} (F1 Macro: {all_metrics[best_model_name][\"f1_macro\"]:.4f})')\n",
    "\n",
    "# Load best model\n",
    "best_model = get_model(\n",
    "    name=best_model_name,\n",
    "    num_classes=config.model.num_classes,\n",
    "    pretrained=False,\n",
    "    dropout=config.model.dropout,\n",
    ")\n",
    "checkpoint = torch.load(\n",
    "    config.train.checkpoint_dir / f'{best_model_name}_best.pth',\n",
    "    map_location=DEVICE, weights_only=True\n",
    ")\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model = best_model.to(DEVICE)\n",
    "best_model.eval()\n",
    "\n",
    "# Get test dataset with transform\n",
    "test_dataset = SplitDataset(splits['test'], transform=val_transform)\n",
    "\n",
    "correct_examples, incorrect_examples = get_prediction_examples(\n",
    "    best_model, test_dataset, dataset.idx_to_class, DEVICE,\n",
    "    num_correct=5, num_incorrect=5,\n",
    ")\n",
    "\n",
    "print(f'\\nCollected {len(correct_examples)} correct and {len(incorrect_examples)} incorrect examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f87668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correct predictions\n",
    "fig = plot_prediction_grid(\n",
    "    correct_examples,\n",
    "    title='Correct Predictions',\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'correct_predictions.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360871c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot incorrect predictions\n",
    "fig = plot_prediction_grid(\n",
    "    incorrect_examples,\n",
    "    title='Incorrect Predictions',\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'incorrect_predictions.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis -- most confused class pairs\n",
    "preds, labels, _ = all_predictions[best_model_name]\n",
    "cm = sk_confusion_matrix(labels, preds)\n",
    "\n",
    "# Work on a copy to avoid modifying the original matrix\n",
    "cm_off_diag = cm.copy()\n",
    "np.fill_diagonal(cm_off_diag, 0)\n",
    "\n",
    "confused_pairs = []\n",
    "for i in range(cm_off_diag.shape[0]):\n",
    "    for j in range(cm_off_diag.shape[1]):\n",
    "        if cm_off_diag[i, j] > 0:\n",
    "            confused_pairs.append((i, j, cm_off_diag[i, j]))\n",
    "\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f'\\nTop 5 Most Confused Class Pairs ({best_model_name}):')\n",
    "print(f'{\"True Class\":<35} {\"Predicted As\":<35} {\"Count\":<10}')\n",
    "print('-' * 80)\n",
    "for true_idx, pred_idx, count in confused_pairs[:5]:\n",
    "    true_name = shorten_class_name(class_names[true_idx])\n",
    "    pred_name = shorten_class_name(class_names[pred_idx])\n",
    "    print(f'{true_name:<35} {pred_name:<35} {count:<10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4f48",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Comparison (Part 3b)\n",
    "\n",
    "Profile all models and compare on accuracy, F1, model size, and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b33e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile all models\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f'\\nProfiling {model_name}...')\n",
    "    \n",
    "    model = get_model(\n",
    "        name=model_name,\n",
    "        num_classes=config.model.num_classes,\n",
    "        pretrained=False,\n",
    "        dropout=config.model.dropout,\n",
    "    )\n",
    "    checkpoint_path = config.train.checkpoint_dir / f'{model_name}_best.pth'\n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    profile = profile_model(model, device=DEVICE, num_warmup=5, num_runs=50)\n",
    "    metrics = all_metrics[model_name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'model': model_name,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'f1_macro': metrics['f1_macro'],\n",
    "        'f1_weighted': metrics['f1_weighted'],\n",
    "        'model_size_mb': profile['model_size_mb'],\n",
    "        'total_params': profile['total_params'],\n",
    "        'cpu_latency_mean_ms': profile['cpu_latency_mean_ms'],\n",
    "        'cpu_latency_p95_ms': profile['cpu_latency_p95_ms'],\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print('\\n' + '='*80)\n",
    "print('MODEL COMPARISON')\n",
    "print('='*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf971b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison charts\n",
    "fig = plot_model_comparison(\n",
    "    comparison_df,\n",
    "    figsize=(14, 6),\n",
    "    save_path=PROJECT_ROOT / 'outputs' / 'model_comparison.png',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d333e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Business Recommendation (Part 3c)\n",
    "\n",
    "### Which model to deploy for a mobile app for farmers?\n",
    "\n",
    "**Recommendation: EfficientNet-B0**\n",
    "\n",
    "For a mobile crop disease detection app, we recommend **EfficientNet-B0** as the deployment model based on the following analysis:\n",
    "\n",
    "| Criterion | ResNet-50 | EfficientNet-B0 | MobileNetV3-Small |\n",
    "|-----------|-----------|-----------------|--------------------|\n",
    "| Model Size | ~98 MB | ~20 MB | ~10 MB |\n",
    "| TFLite INT8 | ~25 MB | ~5 MB | ~2.5 MB |\n",
    "| CPU Inference | Slowest | Moderate | Fastest |\n",
    "| Accuracy | Highest | High (within 1-2%) | Lower (2-3% drop) |\n",
    "\n",
    "**Why not ResNet-50?** At ~98 MB, it is too large for mobile deployment. The marginal accuracy gain (~1%) does not justify the 5x larger model size and 3x higher inference latency on mobile devices.\n",
    "\n",
    "**Why not MobileNetV3-Small?** While ultra-lightweight (~10 MB), the 2-3% accuracy drop translates to missed diseases in production. In agriculture, a false negative (missing a disease) can lead to crop loss and incorrect treatment, directly impacting farmer outcomes .\n",
    "\n",
    "**EfficientNet-B0 is the sweet spot:** It achieves competitive accuracy while being small enough for mobile deployment (~5 MB after INT8 quantization). The deployment pipeline would be: PyTorch (.pth) -> ONNX (.onnx) -> TFLite (.tflite) with INT8 quantization, running entirely on-device for offline functionality.\n",
    "\n",
    "### Confidence Thresholding Strategy\n",
    "Predictions with confidence below 70% should be suppressed with a message: \"Low confidence. Please retake the photo with better lighting and a clear view of the leaf.\" This prevents the worst UX failure: a confidently wrong diagnosis leading to incorrect crop treatment.\n",
    "\n",
    "### Known Limitations\n",
    "1. **Lab-to-field domain gap**: Model trained on clean lab images may underperform on messy field photos\n",
    "2. **Single-disease assumption**: Cannot detect co-infections\n",
    "3. **Limited crop coverage**: Only 3 crops / 12 classes (same pipeline scales to more)\n",
    "4. **No severity grading**: Detects disease type but not progression stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6165476",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export\n",
    "\n",
    "Save best model checkpoint, class mapping, and all figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export class mapping\n",
    "class_mapping = {str(idx): name for idx, name in dataset.idx_to_class.items()}\n",
    "\n",
    "config.train.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mapping_path = config.train.checkpoint_dir / 'class_mapping.json'\n",
    "with open(mapping_path, 'w') as f:\n",
    "    json.dump(class_mapping, f, indent=2)\n",
    "print(f'Class mapping saved to {mapping_path}')\n",
    "\n",
    "# Print class mapping\n",
    "print('\\nClass Mapping:')\n",
    "for idx, name in sorted(class_mapping.items(), key=lambda x: int(x[0])):\n",
    "    print(f'  {idx}: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31be374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "config_dict = {\n",
    "    'data': {\n",
    "        'raw_data_dir': str(config.data.raw_data_dir),\n",
    "        'image_size': config.data.image_size,\n",
    "        'selected_classes': config.data.selected_classes,\n",
    "        'split_ratios': config.data.split_ratios,\n",
    "        'random_seed': config.data.random_seed,\n",
    "    },\n",
    "    'train': {\n",
    "        'batch_size': config.train.batch_size,\n",
    "        'stage1_epochs': config.train.stage1_epochs,\n",
    "        'stage1_lr': config.train.stage1_lr,\n",
    "        'stage2_epochs': config.train.stage2_epochs,\n",
    "        'stage2_lr': config.train.stage2_lr,\n",
    "        'stage3_epochs': config.train.stage3_epochs,\n",
    "        'stage3_lr': config.train.stage3_lr,\n",
    "        'weight_decay': config.train.weight_decay,\n",
    "        'label_smoothing': config.train.label_smoothing,\n",
    "        'early_stopping_patience': config.train.early_stopping_patience,\n",
    "        'scheduler': config.train.scheduler,\n",
    "    },\n",
    "    'model': {\n",
    "        'num_classes': config.model.num_classes,\n",
    "        'dropout': config.model.dropout,\n",
    "        'pretrained': config.model.pretrained,\n",
    "    },\n",
    "    'results': {\n",
    "        model_name: all_metrics[model_name]\n",
    "        for model_name in model_names\n",
    "        if model_name in all_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = config.train.checkpoint_dir / 'training_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(f'Configuration saved to {config_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbaca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all outputs\n",
    "print('\\n' + '=' * 60)\n",
    "print('EXPORT SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "outputs_dir = PROJECT_ROOT / 'outputs'\n",
    "models_dir = config.train.checkpoint_dir\n",
    "\n",
    "print('\\nSaved Models:')\n",
    "if models_dir.exists():\n",
    "    for f in sorted(models_dir.iterdir()):\n",
    "        size_mb = f.stat().st_size / (1024 ** 2)\n",
    "        print(f'  {f.name} ({size_mb:.1f} MB)')\n",
    "\n",
    "print('\\nSaved Figures:')\n",
    "if outputs_dir.exists():\n",
    "    for f in sorted(outputs_dir.iterdir()):\n",
    "        if f.suffix == '.png':\n",
    "            print(f'  {f.name}')\n",
    "\n",
    "print('\\nFinal Model Performance:')\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    print(f'  {model_name:20s}: Acc={metrics[\"accuracy\"]:.4f}, F1={metrics[\"f1_macro\"]:.4f}')\n",
    "\n",
    "print('\\nAll exports complete!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
